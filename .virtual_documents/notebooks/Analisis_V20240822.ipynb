


# Import libraries 

# Data Manipulation
import pandas as pd
import numpy as np

# Plots
import seaborn as sns
import matplotlib.pyplot as plt

# Data preprocessing
from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from scipy import stats
from sklearn.feature_extraction import FeatureHasher
from sklearn.compose import ColumnTransformer

# Data splitting
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import StratifiedKFold

# Feature Selection
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor

# Models
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import ExtraTreesRegressor , AdaBoostRegressor , BaggingRegressor
from sklearn.tree import DecisionTreeRegressor , ExtraTreeRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.neural_network import MLPRegressor

# Feature Selection
from xgboost import XGBClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.feature_selection import RFECV , RFE
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import mutual_info_regression

# Metrics
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import cross_val_score

# Hyperparameter optimization
from sklearn.model_selection import GridSearchCV

# Data imputation
from sklearn.impute import KNNImputer
from sklearn.impute import SimpleImputer

# Feature importance
import shap

# Genetic Algorithm
from deap import base, creator, tools, algorithms

# Utils
from itertools import product
import json
import time
import warnings
import pickle
from scipy.interpolate import make_interp_spline
from scipy.interpolate import interp1d
import pickle as pkl
from sklearn.pipeline import Pipeline , make_pipeline
import random
import joblib
warnings.filterwarnings("ignore")
# Set Numpy Seed
np.random.seed(0)
#plt.style.use('ggplot')
print('Libraries imported.')





# Cargar información
ruta_archivo = r'..\data'
nombre_archivo = '\Resultados__nicos_Saber_Pro_20231019.csv'
tipo_de_dato = {'ESTU_CODDANE_COLE_TERMINO' : str}
data_1 = pd.read_csv(ruta_archivo + nombre_archivo).reset_index(drop = True)
# Dejar solo los resultados del programa de ingeniería industrial
filtro_academico = ['INGENIERIA INDUSTRIAL','INGENIERÍA INDUSTRIAL']
data_1 = data_1[data_1['ESTU_PRGM_ACADEMICO'].isin(filtro_academico)]
data_1


# Definir las columnas de los puntajes
columnas_puntajes = ['MOD_RAZONA_CUANTITAT_PUNT',
                     'MOD_COMUNI_ESCRITA_PUNT',
                     'MOD_INGLES_PUNT',
                     'MOD_LECTURA_CRITICA_PUNT',
                     'MOD_COMPETEN_CIUDADA_PUNT']
# Quitar los valores nulos de las columnas de puntaje
data = data_1.copy()
for i in columnas_puntajes:
    data = data[data[i].isnull() == False]
data


# Crear la columna de target como el promedio de todos los puntajes
data['PUNTAJE_GLOBAL'] = data[columnas_puntajes].mean(axis = 1)
data


# Mostrar columnas
data.columns.tolist()


# Mostrar el número de instituciones por año, evaluadas.
aux_periodos = pd.DataFrame()
for i in data['PERIODO'].unique().tolist():
    aux = pd.DataFrame({'Periodo' : [str(i)[:4]  + str(i)[-1:].zfill(2)],
                        'Total Instituciones' :len(data[data['PERIODO'] == i]['INST_NOMBRE_INSTITUCION'].unique().tolist())})
    aux_periodos = pd.concat([aux_periodos,
                              aux] , axis = 0)
aux_periodos.sort_values(by = 'Periodo')


# Definir Target y tipos de datos
target = ['PUNTAJE_GLOBAL'] +  ['MOD_RAZONA_CUANTITAT_PUNT',
                                 'MOD_COMUNI_ESCRITA_PUNT',
                                 'MOD_INGLES_PUNT',
                                 'MOD_LECTURA_CRITICA_PUNT',
                                 'MOD_COMPETEN_CIUDADA_PUNT']
ignore_columns = [ 'PERIODO', 'ESTU_CONSECUTIVO', 'ESTU_TIPODOCUMENTO' ,'ESTU_PAIS_RESIDE', 'ESTU_DEPTO_RESIDE', 'ESTU_MCPIO_RESIDE',
                  'ESTU_COD_COLE_MCPIO_TERMINO', 'INST_NOMBRE_INSTITUCION' ,'ESTU_NUCLEO_PREGRADO' , 'ESTU_INST_DEPARTAMENTO' , 'ESTU_INST_CODMUNICIPIO',
                  'ESTU_INST_MUNICIPIO' , 'ESTU_PRGM_DEPARTAMENTO' , 'ESTU_PRGM_CODMUNICIPIO' , 'ESTU_PRGM_MUNICIPIO' , 'ESTU_DEPTO_PRESENTACION' , 'ESTU_COD_MCPIO_PRESENTACION',
                  'ESTU_MCPIO_PRESENTACION' , 'ESTU_NACIONALIDAD' , 'ESTU_ESTUDIANTE' , 'ESTU_COLE_TERMINO' , 'ESTU_TIPODOCUMENTOSB11' , 'ESTU_FECHANACIMIENTO',
                  'ESTU_CODDANE_COLE_TERMINO' , 'MOD_INGLES_DESEM' , 'ESTU_NIVEL_PRGM_ACADEMICO' , 'ESTU_PRIVADO_LIBERTAD' , 'MOD_COMUNI_ESCRITA_DESEM',
                 'ESTU_COD_RESIDE_MCPIO' , 'ESTU_PRGM_ACADEMICO', 'ESTU_SNIES_PRGMACADEMICO']
ordinal_features = ['ESTU_VALORMATRICULAUNIVERSIDAD' , 'ESTU_HORASSEMANATRABAJA' , 'FAMI_ESTRATOVIVIENDA',
                   'FAMI_EDUCACIONPADRE' , 'FAMI_EDUCACIONMADRE']
numeric_features = []
categorical_features = [i for i in data.columns.tolist() if i not in target and i not in numeric_features and i not in ignore_columns and i not in ordinal_features]
categorical_features





# Quitar las columnas que se ignoran
data = data.drop(columns = ignore_columns)


# Describe data
data.info()


data.describe()


# Mostrar valores nulos
null_values_count = pd.DataFrame(data.isnull().sum() / data.shape[0]).sort_values(by = 0 , ascending = False).T
null_values_count = null_values_count.T.reset_index()
null_values_count.columns = ['Variable' , '% de nulos']
null_values_count.index = null_values_count['Variable']
null_values_count = null_values_count.drop(columns = ['Variable'])
null_values_count['% de nulos'] = round(null_values_count['% de nulos'] * 100 , 2)
null_values_count


# Quitar categorias con más del 10% de nans
to_drop_features = [i for i in null_values_count.T.columns.tolist() if null_values_count.T[i].values[0] > 10]
data = data.drop(columns = to_drop_features)
# Re definir variables
categorical_features = [i for i in categorical_features if i in data.columns.tolist()]
numeric_features = [i for i in numeric_features if i in data.columns.tolist()]
print('Variables Eliminadas:')
to_drop_features


# Mostrar distribución del target
for i in target:
    plt.figure()
    sns.distplot(data[i])
    plt.title('Distribución del target ' + str(i))
    plt.legend([f'Media: {round(data[i].values.mean())}\nDesviación Estandar: {round(data[i].values.std())}\nMáximo: {round(data[i].values.max())}'], loc='upper left')
    plt.ylabel('Frecuencia')
    plt.show()


# Realizar gráfico de distribución del target por cada una de las variables categóricas y ordinales
excluir_grafico = ['ESTU_COD_RESIDE_DEPTO' , 'ESTU_COD_RESIDE_MCPIO' , 'ESTU_COD_DEPTO_PRESENTACION' , 'INST_COD_INSTITUCION' , 'ESTU_SNIES_PRGMACADEMICO',
                  'ESTU_PRGM_ACADEMICO']
for i in ['PUNTAJE_GLOBAL']:
    for j in data.columns.tolist():
        if j not in numeric_features and j not in target and j not in excluir_grafico:
            # Realizar la prueba de chi-cuadrado
            contingency_table = pd.crosstab(data[i], data[j])
            chi2, p, dof, ex = stats.chi2_contingency(contingency_table)
            sns.displot(data, x = i, hue = j)
            plt.title(f'Distribución de {i} con respecto a la variable {j}\nChi2 p-valor: {p:.4f}')
            plt.ylabel('Número Observaciones')
            plt.show()


# Codificar las variables oridnales
codificador_ordinales = pd.read_excel('..\data\Parametros.xlsx' ,
                                      sheet_name = 'Codificacion_Ordinales',
                                      dtype = {'COLUMNA' : str , 
                                                'VALOR_ACTUAL' : str})
for i in ordinal_features:
    print('Categorizando variable -->' , i)
    aux = codificador_ordinales[codificador_ordinales['COLUMNA'] == i]
    data[i + '_ENCODED'] = data[i].map(dict(aux[['VALOR_ACTUAL' , 'VALOR_CODIFICADO']].values))
# Actualizar las variables ordinales
ordinal_features = [i for i in data.columns.tolist() if '_ENCODED' in i]
print('Nuevas variables oridnales:' , ordinal_features)
data


# Mostrar valores variables categoricas
for i in list(data.columns):
    if i not in numeric_features and i not in target:
        print('-' * 50)
        print(f'{i :-<20} : {data[i].unique()} : Total valores unicos : {len(data[i].unique().tolist())}')


# Horizontal bar plot for cateogorical features
for i in list(data.columns):
    if i in categorical_features or i in ordinal_features:
        print('\n------------------------------------\n')
        data.groupby(i)[target[0]].count().plot(kind = 'barh' , title = 'Distribution of ' + i)
        plt.show()





# Aplicar one hot encoding a las variables categóricas
numeric_scaler = MinMaxScaler()
cat_encoder = OrdinalEncoder()
# Definir una base con las variables que son categoricas pero no se necesitan hacer one hot encoding
categorical_no_encoding = ['ESTU_COD_RESIDE_DEPTO',
                             'ESTU_COD_DEPTO_PRESENTACION',
                             'INST_COD_INSTITUCION']
categorical_yes_encoding = [i for i in categorical_features if i not in categorical_no_encoding]
# Fit objects
#numeric_scaler.fit(data_1[numeric_features])
cat_encoder.fit(data[categorical_yes_encoding])
# Transformar la información
#aux_numeric = pd.DataFrame(numeric_scaler.transform(data_1[numeric_features]) , columns = data_1[numeric_features].columns.tolist())
#aux_categorical = pd.DataFrame.sparse.from_spmatrix(cat_encoder.transform(data[categorical_features]) , columns = cat_encoder.get_feature_names_out().tolist())
aux_categorical = pd.DataFrame(cat_encoder.transform(data[categorical_yes_encoding]) , columns = categorical_yes_encoding)
# Concatenar la información
data_2 = pd.concat([aux_categorical.reset_index(drop = True),
                    data[ordinal_features].reset_index(drop = True),
                    data[categorical_no_encoding].reset_index(drop = True),
                    data[target].reset_index(drop = True)] , axis = 1)
# Definir los tipos de datos para las variables
for i in data_2.columns.tolist():
    print('-' * 25)
    print(i)
    if i in numeric_features or i in target:
        data_2[i] = data_2[i].fillna(-1).astype(float)
        print('Variable' , i , 'converdita a float')
    if i in ordinal_features or i in categorical_features:
        data_2[i] = data_2[i].fillna(-1).astype(int)
        print('Variable' , i , 'convertida a int')
data_2





# Generate all posible combinations of steps
POSIBLE_VALUES = {'Imputation' : [SimpleImputer(strategy = 'mean'),
                                  KNNImputer()],
                  'Seleccion_Variables' : [SelectKBest(score_func=f_regression, k=10),
                                           SelectFromModel(estimator=AdaBoostRegressor()),
                                           VarianceThreshold(threshold=0.1),
                                           SelectKBest(score_func=mutual_info_regression, k=10),
                                          'passthrough'],
                 'Model' : [ExtraTreesRegressor(),
                            ExtraTreeRegressor(),
                            AdaBoostRegressor(),
                            XGBRegressor(),
                            BaggingRegressor(),
                            DecisionTreeRegressor(),
                            CatBoostRegressor(logging_level = 'Silent'),
                            LGBMRegressor(verbosity=-1),
                            HistGradientBoostingRegressor(),
                            MLPRegressor(hidden_layer_sizes = (512 , 10))]}

num_combinations = len(list(product(*POSIBLE_VALUES.values())))
all_combinations = [
    {key: value for key, value in zip(POSIBLE_VALUES.keys(), combo)}
    for combo in product(*POSIBLE_VALUES.values())]
print('Total Combinations:' , num_combinations)





aux_target = target[0]
aux_target


# Seleccionar la combinación de opciones usando algoritmos genéticos
X = data_2.drop(columns = target)
Y = data_2[aux_target]
print('X.shape' , X.shape)
print('Y.shape' , Y.shape)
# Definir la función de fitness
def evaluate(individual):

    # Crear pipeline con los componentes seleccionados
    steps = [
        ('imputer', POSIBLE_VALUES['Imputation'][individual[0]]),
        ('feature_selection' , POSIBLE_VALUES['Seleccion_Variables'][individual[1]]),
        ('regressor', POSIBLE_VALUES['Model'][individual[2]])
    ]
    
    pipeline = Pipeline(steps)
    
    # Entrenar y evaluar con MAE
    scores = cross_val_score(pipeline, X, Y, cv = 3, scoring='neg_mean_absolute_error')
    return np.mean(scores) * -1,

# Create the genetic algorithm's toolbox
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

toolbox = base.Toolbox()
toolbox.register("attr_imp", random.randint, 0, len(POSIBLE_VALUES['Imputation']) - 1)
toolbox.register("attr_feature_selector", random.randint, 0, len(POSIBLE_VALUES['Seleccion_Variables']) - 1)
toolbox.register("attr_model", random.randint, 0, len(POSIBLE_VALUES['Model']) - 1)

# Add the seed as a fifth attribute
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_imp, toolbox.attr_feature_selector, toolbox.attr_model), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", evaluate)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutUniformInt, low=[0]*3, up=[len(POSIBLE_VALUES['Imputation']) - 1,
                                                               len(POSIBLE_VALUES['Seleccion_Variables']) - 1,
                                                               len(POSIBLE_VALUES['Model']) - 1],
                indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the genetic algorithm
def genetic_algorithm(n_gen=5, pop_size=10, cxpb=0.5, mutpb=0.2):
    pop = toolbox.population(n=pop_size)
    hof = tools.HallOfFame(1)
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", np.mean)
    stats.register("std", np.std)
    stats.register("min", np.min)
    stats.register("max", np.max)

    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=n_gen,
                                       stats=stats, halloffame=hof, verbose=True)
    
    return pop, hof, logbook

# Execute the genetic algorithm
population, hall_of_fame, logbook = genetic_algorithm()

# Display the best individual found
best_individual = hall_of_fame[0]
print("Best individual is:", best_individual)
print('Best selection is:\n' ,
      POSIBLE_VALUES['Imputation'][best_individual[0]] , '\n' ,
      POSIBLE_VALUES['Seleccion_Variables'][best_individual[1]] , '\n' ,
      POSIBLE_VALUES['Model'][best_individual[2]])
print("Mejor MAE:", evaluate(best_individual))


# Crear un pipeline con la mejor seleccion  
mejor_pipeline = make_pipeline(POSIBLE_VALUES['Imputation'][best_individual[0]],
                          POSIBLE_VALUES['Seleccion_Variables'][best_individual[1]],
                          POSIBLE_VALUES['Model'][best_individual[2]])
mejor_pipeline.named_steps


# Definir los parámetros a optimizar
parametros_optimizar = {'knnimputer__n_neighbors' : [5 , 50, 100  , 500 , 1000],
                       'variancethreshold__threshold' : [0.1 , 0.25 , 0.5],
                       'catboostregressor__depth' : [6 , 8 , 10],
                       'catboostregressor__learning_rate' : [0.01 , 0.05 , 0.1]}

# Crear el grid search para realizar la búsqueda de hyperparametros
grid_search = GridSearchCV(mejor_pipeline,
                          param_grid = parametros_optimizar,
                          cv = 5,
                          verbose = 10,
                          scoring = 'neg_mean_absolute_error')
grid_search.fit(X , Y)


# Mostrar los resultados
print('Mejor modelo' , grid_search.best_estimator_)
print('Mejores parametros' , grid_search.best_params_)
print('Mejor MAE' , grid_search.best_score_)
pd.DataFrame(grid_search.cv_results_)


# Guardar resultados
pd.DataFrame(grid_search.cv_results_).to_excel(r'..\results\Resultados_Grid_Search_V20240822.xlsx')

# Guardar el modelo optimizado
# Exportar el categorical encoder
with open(r'..\models\Pipeline_V20240822.pkl' , 'wb') as file:
    joblib.dump(grid_search , file)
print('Archivo Generado -->' , str(grid_search) , 'en ../models/Pipeline_V20240822.pkl')





# Obtener el mejor modelo del grid search
mejor_modelo = grid_search.best_estimator_
feature_names = X.columns

# Aplicar las transformaciones sin el modelo final
X_transformed = mejor_modelo[:-1].transform(X)

# Filtrar los nombres de las características que no fueron eliminadas por VarianceThreshold
mask = mejor_modelo.named_steps['variancethreshold'].get_support()
selected_features = feature_names[mask].tolist()

# Ahora selected_features contiene los nombres de las variables que se utilizan en CatBoost
print('Variable Seleccionadas:', selected_features)


# Seleccionar para explicar sólo el dataset de los estudiantes de Popayán
ordinal_features = ['ESTU_VALORMATRICULAUNIVERSIDAD' , 'ESTU_HORASSEMANATRABAJA' , 'FAMI_ESTRATOVIVIENDA',
                   'FAMI_EDUCACIONPADRE' , 'FAMI_EDUCACIONMADRE']
dataset_popayan = pd.read_excel(r'..\data\Datos_Popayan.xlsx')
for i in ordinal_features:
    aux = codificador_ordinales[codificador_ordinales['COLUMNA'] == i]
    dataset_popayan[i + '_ENCODED'] = dataset_popayan[i].map(dict(aux[['VALOR_ACTUAL' , 'VALOR_CODIFICADO']].values))
# Actualizar las variables ordinales
ordinal_features = [i for i in dataset_popayan.columns.tolist() if '_ENCODED' in i]
# Aplicar el Ordinal Encoder
aux_categorical = pd.DataFrame(cat_encoder.transform(dataset_popayan[categorical_yes_encoding]) , columns = categorical_yes_encoding)
# Concatenar la información
data_3 = pd.concat([aux_categorical.reset_index(drop = True),
                    dataset_popayan[ordinal_features].reset_index(drop = True),
                    dataset_popayan[categorical_no_encoding].reset_index(drop = True),
                    dataset_popayan[target].reset_index(drop = True)] , axis = 1)
# Definir los tipos de datos para las variables
for i in data_3.columns.tolist():
    if i in numeric_features or i in target:
        data_3[i] = data_3[i].fillna(-1).astype(float)
    if i in ordinal_features or i in categorical_features:
        data_3[i] = data_3[i].fillna(-1).astype(int)

X_popayan = data_3.drop(columns = target)
        
# Transformar los datos con el pipeline       
X_transformed = mejor_modelo[:-1].transform(X_popayan)

# Obtener la explicación de SHAP
explainer = shap.KernelExplainer(mejor_modelo.named_steps['catboostregressor'].predict, X_transformed)

# Calcular los valores SHAP para la instancia seleccionada
fila = 200
shap_values = explainer(X_transformed[fila : fila + 1, :])

# Colocar los labels originales
explicacion = shap.Explanation(shap_values.values, 
                  shap_values.base_values, 
                  data = X_transformed, 
                  feature_names = selected_features)

# Crear el waterfall plot
shap.plots.waterfall(explicacion[0])


# Importancia generalizada de variables
X_popayan = data_3.drop(columns = target)
        
# Transformar los datos con el pipeline       
X_transformed = mejor_modelo[:-1].transform(X_popayan)

# Obtener la explicación de SHAP
explainer = shap.KernelExplainer(mejor_modelo.named_steps['catboostregressor'].predict, shap.sample(X_transformed , 50))

# Calcular los valores SHAP para la instancia seleccionada
shap_values = explainer(X_transformed)

# Colocar los labels originales
explicacion = shap.Explanation(shap_values.values, 
                  shap_values.base_values, 
                  data = X_transformed, 
                  feature_names = selected_features)
shap.plots.bar(explicacion)


# Exportar los valores shap
with open(r'..\models\shap_values.pkl', 'wb') as f:
    pickle.dump({'shap_values': shap_values.values, 'base_values': shap_values.base_values}, f)


# Ejemplo de cómo cargarlos y volverlos a usar
with open(r'..\models\shap_values.pkl', 'rb') as f:
    data = pickle.load(f)
    loaded_shap_values = data['shap_values']
    loaded_base_values = data['base_values']

# Crear un SHAP value object usando los valores cargados
shap_values_object = shap.Explanation(loaded_shap_values,
                                      base_values=loaded_base_values,
                                      feature_names = selected_features,
                                     data = X_transformed)

# Realizar un waterfall plot para una instancia específica
shap.waterfall_plot(shap_values_object[10])
shap.plots.bar(shap_values_object)
